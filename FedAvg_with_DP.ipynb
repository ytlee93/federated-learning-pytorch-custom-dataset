{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import syft as sy\n",
    "import copy\n",
    "import numpy as np\n",
    "import time\n",
    "from opacus import PrivacyEngine\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from torchsummary import summary\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "import importlib\n",
    "importlib.import_module('FLDataset')\n",
    "from FLDataset import load_dataset, getActualImgs, CovidDataset, Rescale, ToTensor\n",
    "from utils import averageModels, averageGradients\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e556b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.images = 3012\n",
    "        self.clients = 3\n",
    "        self.rounds = 1001\n",
    "        self.epochs = 1\n",
    "        self.local_batches = 20\n",
    "        self.lr = 0.01\n",
    "        self.dropout1 = 0.25\n",
    "        self.dropout2 = 0.5\n",
    "        self.C = 0.66\n",
    "        self.drop_rate = 0.1\n",
    "        self.torch_seed = 0\n",
    "        self.log_interval = 10\n",
    "        self.iid = 'noniid'\n",
    "        self.split_size = int(self.images / self.clients)\n",
    "        self.samples = self.split_size / self.images \n",
    "        self.use_cuda = True\n",
    "        # save model \n",
    "        self.save_model = False\n",
    "        self.save_model_interval = 500\n",
    "        # clip grad norm\n",
    "        self.clip = 1\n",
    "        # delete tensorboard record\n",
    "        self.del_runs = False\n",
    "        # accuracy csv file \n",
    "        self.acc_csv = False\n",
    "        self.acc_file = '0514_3clients_withDP.csv'\n",
    "        # number of classes per client on non iid case \n",
    "        self.noniid_classnum = 2\n",
    "        # data transform\n",
    "        self.transform = transforms.Compose([Rescale(32), ToTensor()])\n",
    "        # number of classes\n",
    "        self.c_num = 3\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = args.use_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete files in runs (Tensorboard)\n",
    "if args.del_runs==True:\n",
    "    folder = 'runs'\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create accuracy csv file\n",
    "def acc_csv(args, rnd, acc):\n",
    "    if args.acc_csv==True:\n",
    "        with open(\"acc_csv_files/\"+args.acc_file, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([rnd, acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dcd75a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "clients = []\n",
    "\n",
    "for i in range(args.clients):\n",
    "    clients.append({'hook': sy.VirtualWorker(hook, id=\"client{}\".format(i+1))})\n",
    "\n",
    "print(clients)\n",
    "print(\"number of clients : \", len(clients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d69371",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_train, global_test, train_group, test_group = load_dataset(args.clients, args.iid, \\\n",
    "                                                                  args.transform, args.c_num, \\\n",
    "                                                                  args.noniid_classnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(global_train))\n",
    "print(type(global_train))\n",
    "print(len(global_test))\n",
    "print(type(global_test))\n",
    "print(len(train_group))\n",
    "print(type(train_group))\n",
    "print(len(test_group))\n",
    "print(type(test_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f90c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for inx, client in enumerate(clients):\n",
    "    trainset_ind_list = list(train_group[inx])\n",
    "    print(\"len(client\", str(inx), \"train set) = \", len(trainset_ind_list))\n",
    "    client['trainset'] = getActualImgs(global_train, trainset_ind_list, args.local_batches)\n",
    "    client['testset'] = getActualImgs(global_test, list(test_group[inx]), args.local_batches)\n",
    "    client['samples'] = len(trainset_ind_list) / args.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf68387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "global_test_dataset = CovidDataset('./test.csv', transform=transforms.Compose([Rescale(32), ToTensor()]))\n",
    "global_test_loader = DataLoader(global_test_dataset, batch_size=args.local_batches, shuffle=True, drop_last=True)\n",
    "print(len(global_test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1,\n",
    "                               out_channels = 32,\n",
    "                               kernel_size = 3,\n",
    "                               stride = 1)\n",
    "#         self.conv1_bn = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32,\n",
    "                               out_channels = 64,\n",
    "                               kernel_size = 3,\n",
    "                               stride = 1)\n",
    "        self.fc1 = nn.Linear(14*14*64, 128)\n",
    "#         self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "#         x = self.conv1_bn(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.dropout(x, p=args.dropout1)\n",
    "        x = x.view(-1, 14*14*64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=args.dropout2)\n",
    "#         x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a77f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClientUpdate(args, device, client):\n",
    "    client['model'].train()\n",
    "#     client['model'].send(client['hook'])\n",
    "    \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for batch_idx, (data, target) in enumerate(client['trainset']):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            client['optim'].zero_grad()\n",
    "#             output = client['model'](data.float())\n",
    "#             loss = F.nll_loss(output, target.squeeze(1))\n",
    "            output = client['model'](data)\n",
    "            loss = client['criterion'](output, target.squeeze(1))\n",
    "            loss.backward()\n",
    "            \n",
    "#             print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "#             print(\"output: \", output)\n",
    "#             print(\"target squeeze: \", target.squeeze(1))\n",
    "#             print(\"loss: \", loss)\n",
    "#             print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "            \n",
    "#             print weight \n",
    "#             for name, param in client['model'].named_parameters():\n",
    "#                 if name=='conv1_bn.weight':\n",
    "#                     print(name, param.grad)\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(client['model'].parameters(), args.clip)\n",
    "            client['optim'].step()\n",
    "            \n",
    "            \n",
    "            if batch_idx % args.log_interval == 0 or batch_idx==len(client['trainset'])-1:\n",
    "#                 loss = loss.get() \n",
    "                print('Model [{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    client['hook'].id,\n",
    "                    epoch, (batch_idx+1) * args.local_batches, len(client['trainset']) * args.local_batches, \n",
    "                    100. * (batch_idx+1) / len(client['trainset']), loss.item()/args.log_interval))\n",
    "                \n",
    "#     client['model'].get() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc236303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader, name):\n",
    "    model.eval()   \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for d in test_loader:\n",
    "            data = d['image']\n",
    "            target = d['label']\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            if(str(device)=='cuda'):\n",
    "                model.cuda()\n",
    "            output = model(data.float())\n",
    "#             test_loss += F.nll_loss(output, target.squeeze(1), reduction='sum').item() # sum up batch loss\n",
    "            loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "            test_loss += loss_fn(output, target.squeeze(1)).item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss for {} model: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        name, test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddefee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.torch_seed)\n",
    "global_model = Net().to(device)\n",
    "summary(global_model, (1, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "for client in clients:\n",
    "    torch.manual_seed(args.torch_seed)\n",
    "    client['model'] = Net().to(device)\n",
    "    client['optim'] = optim.SGD(client['model'].parameters(), lr=args.lr, momentum = 0.8)\n",
    "    client['criterion'] = nn.CrossEntropyLoss(reduction='mean')\n",
    "    client['pengine'] = PrivacyEngine(\n",
    "                                       client['model'],\n",
    "                                       batch_size=args.local_batches,\n",
    "                                       sample_size=len(client['trainset']),\n",
    "                                       alphas=range(2,32),\n",
    "                                       noise_multiplier=0.3,\n",
    "                                       max_grad_norm=1.0\n",
    "                                    )\n",
    "    client['pengine'].attach(client['optim']) \n",
    "    \n",
    "# start training model\n",
    "training_start_time = time.time()\n",
    "for fed_round in range(args.rounds):\n",
    "    print(\"\")\n",
    "    print(\"===================================================================\")\n",
    "    print(\"[round] = \", fed_round+1, \"/\", args.rounds)\n",
    "    print(\"===================================================================\")\n",
    "    \n",
    "    round_train_start_time = time.time()\n",
    "    \n",
    "#     uncomment if you want a randome fraction for C every round\n",
    "#     args.C = float(format(np.random.random(), '.1f'))    \n",
    "    \n",
    "    # number of selected clients\n",
    "    m = int(max(math.ceil(args.C * args.clients), 1))\n",
    "\n",
    "    # Selected devices\n",
    "    np.random.seed(fed_round)\n",
    "    selected_clients_inds = np.random.choice(range(len(clients)), m, replace=False)\n",
    "    selected_clients = [clients[i] for i in selected_clients_inds]\n",
    "    \n",
    "    # Active devices\n",
    "#     np.random.seed(fed_round)\n",
    "#     active_clients_inds = np.random.choice(selected_clients_inds, int((1-args.drop_rate) * m), replace=False)\n",
    "#     active_clients = [clients[i] for i in active_clients_inds]\n",
    "    active_clients = selected_clients\n",
    "    \n",
    "    # Training \n",
    "    client_cnt = 0\n",
    "    for client in active_clients:\n",
    "        print(\"* [client count] = \", client_cnt+1 , \"/\", len(active_clients))\n",
    "        client_train_start_time = time.time()\n",
    "        ClientUpdate(args, device, client)\n",
    "        client_cnt += 1\n",
    "        client_train_time = round(time.time()-client_train_start_time)\n",
    "        print(\"* [client_train_time] = \", str(timedelta(seconds=(client_train_time))))\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "#         # Testing \n",
    "#         for client in active_clients:\n",
    "#             test(args, client['model'], device, client['testset'], client['hook'].id)\n",
    "    \n",
    "    # Averaging \n",
    "#     print(\"active clients: \", active_clients)\n",
    "    global_model = averageModels(global_model, active_clients)\n",
    "    \n",
    "    # Testing the average model\n",
    "    acc = test(args, global_model, device, global_test_loader, 'Global')\n",
    "    writer.add_scalar(\"Accuracy/train\", acc, fed_round)\n",
    "    writer.flush()\n",
    "    acc_csv(args, fed_round, acc)\n",
    "            \n",
    "    # Share the global model with the clients\n",
    "    for client in clients:\n",
    "        client['model'].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "    # training time per round\n",
    "    total_train_time = round(time.time()-training_start_time)\n",
    "    round_train_time = round(time.time()-round_train_start_time)\n",
    "    print(\"** [total train time]: \", str(timedelta(seconds=total_train_time)))\n",
    "    print(\"** [round train time]: \", str(timedelta(seconds=round_train_time)))\n",
    "    \n",
    "    if (args.save_model and fed_round%args.save_model_interval==0 and fed_round!=0):\n",
    "        now = datetime.now() \n",
    "        date = now.strftime(\"%Y_%m_%d_%H%M\")\n",
    "        torch.save(global_model.state_dict(), date + \"_FedAvg_with_DP_round_\" + str(fed_round) + \".pth\")\n",
    "        print(\"model saved : \"+ date +\"_FedAvg_with_DP_round_\" + str(fed_round) + \"10clients.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7710a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard open \n",
    "# tensorboard --logdir=/home/citi302/Desktop/Codefolder/FL_DP_covid/runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
